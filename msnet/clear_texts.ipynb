{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.utils\n",
    "from pandarallel import pandarallel\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(nb_workers=10,progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ürünü hepsiburadadan alalı 3 hafta oldu. orjin...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>urun_yorumlari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ürünlerden çok memnunum, kesinlikle herkese ta...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>urun_yorumlari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hızlı kargo, temiz alışveriş.teşekkür ederim.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>urun_yorumlari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Çünkü aranan tapınak bu bölgededir .</td>\n",
       "      <td>Notr</td>\n",
       "      <td>wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bu telefonu başlıca alma nedenlerim ise elimde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>urun_yorumlari</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label         dataset\n",
       "0  ürünü hepsiburadadan alalı 3 hafta oldu. orjin...  Positive  urun_yorumlari\n",
       "1  ürünlerden çok memnunum, kesinlikle herkese ta...  Positive  urun_yorumlari\n",
       "2      hızlı kargo, temiz alışveriş.teşekkür ederim.  Positive  urun_yorumlari\n",
       "3               Çünkü aranan tapınak bu bölgededir .      Notr            wiki\n",
       "4  bu telefonu başlıca alma nedenlerim ise elimde...  Positive  urun_yorumlari"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "df_train = pd.read_csv(\"hf://datasets/winvoker/turkish-sentiment-analysis-dataset/\" + splits[\"train\"])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.convert_dtypes(convert_string=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train[\"text\"]!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153825\n",
      "235949\n",
      "50905\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train[df_train[\"label\"]==\"Notr\"]))\n",
    "print(len(df_train[df_train[\"label\"]==\"Positive\"]))\n",
    "print(len(df_train[df_train[\"label\"]==\"Negative\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_positive = df_train[df_train[\"label\"]==\"Positive\"].sample(50905,random_state=42)\n",
    "df_train_negative = df_train[df_train[\"label\"]==\"Negative\"].sample(50905,random_state=42)\n",
    "df_train_notr = df_train[df_train[\"label\"]==\"Notr\"].sample(50905,random_state=42)\n",
    "df_normalized_train : pd.DataFrame = sklearn.utils.shuffle(pd.concat((df_train_negative,df_train_positive,df_train_notr),ignore_index=True),random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kral akbaba dikkat çekici renklere sahiptir .</td>\n",
       "      <td>Notr</td>\n",
       "      <td>wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ısrarla korkutmayı başarıyor. sanki korku çok...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>HUMIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neşe ve Üzüntü köprünün kırılmaya başlamasıyla...</td>\n",
       "      <td>Notr</td>\n",
       "      <td>wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i phone 5 ten sonra gene 4'' ekranı tercih ett...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>urun_yorumlari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beşinci sezonda diziye yeni oyuncular katıldı .</td>\n",
       "      <td>Notr</td>\n",
       "      <td>wiki</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label         dataset\n",
       "0      Kral akbaba dikkat çekici renklere sahiptir .      Notr            wiki\n",
       "1   ısrarla korkutmayı başarıyor. sanki korku çok...  Positive           HUMIR\n",
       "2  Neşe ve Üzüntü köprünün kırılmaya başlamasıyla...      Notr            wiki\n",
       "3  i phone 5 ten sonra gene 4'' ekranı tercih ett...  Positive  urun_yorumlari\n",
       "4    Beşinci sezonda diziye yeni oyuncular katıldı .      Notr            wiki"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"hf://datasets/winvoker/turkish-sentiment-analysis-dataset/\" + splits[\"test\"])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.convert_dtypes(convert_string=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85855</th>\n",
       "      <td>öyle böyle bir büyüklük değil, bazı hareketler...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>urun_yorumlari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94508</th>\n",
       "      <td>sıvı detarjanların hemen hemen hepsini denedim...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>urun_yorumlari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131917</th>\n",
       "      <td>araçlar hem ülke içinde hem de ülke dışında sa...</td>\n",
       "      <td>Notr</td>\n",
       "      <td>wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71544</th>\n",
       "      <td>bosch-siemens yerli üretimi makinamızda rakibi...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>urun_yorumlari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41678</th>\n",
       "      <td>topaklanmasi cok kotu 2 tane kedimiz var temiz...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>urun_yorumlari</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label  \\\n",
       "85855   öyle böyle bir büyüklük değil, bazı hareketler...  Positive   \n",
       "94508   sıvı detarjanların hemen hemen hepsini denedim...  Positive   \n",
       "131917  araçlar hem ülke içinde hem de ülke dışında sa...      Notr   \n",
       "71544   bosch-siemens yerli üretimi makinamızda rakibi...  Positive   \n",
       "41678   topaklanmasi cok kotu 2 tane kedimiz var temiz...  Negative   \n",
       "\n",
       "               dataset  \n",
       "85855   urun_yorumlari  \n",
       "94508   urun_yorumlari  \n",
       "131917            wiki  \n",
       "71544   urun_yorumlari  \n",
       "41678   urun_yorumlari  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized_train['text'] = [token.lower() for token in df_normalized_train['text']]\n",
    "df_normalized_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove @ mentions and hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       63\n",
       "label      63\n",
       "dataset    63\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found = df_normalized_train[df_normalized_train['text'].str.contains('@')]\n",
    "found.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 152715 entries, 85855 to 121958\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   text     152715 non-null  object\n",
      " 1   label    152715 non-null  string\n",
      " 2   dataset  152715 non-null  string\n",
      "dtypes: object(1), string(2)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_normalized_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       9\n",
       "label      9\n",
       "dataset    9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized_train['text'] = df_normalized_train['text'].replace('@[A-Za-z0-9]+', '', regex=True).replace('@[A-Za-z0-9]+', '', regex=True)\n",
    "found = df_normalized_train[df_normalized_train['text'].str.contains('@')]\n",
    "found.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       35\n",
       "label      35\n",
       "dataset    35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found = df_normalized_train[df_normalized_train['text'].str.contains('http')]\n",
    "found.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       0\n",
       "label      0\n",
       "dataset    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized_train['text'] = df_normalized_train['text'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "found = df_normalized_train[df_normalized_train['text'].str.contains('http')]\n",
    "found.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152715, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctations & Emojies & Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85855     öyle böyle bir büyüklük değil, bazı hareketler...\n",
       "94508     sıvı detarjanların hemen hemen hepsini denedim...\n",
       "131917    araçlar hem ülke içinde hem de ülke dışında sa...\n",
       "71544     bosch-siemens yerli üretimi makinamızda rakibi...\n",
       "41678     topaklanmasi cok kotu  tane kedimiz var temizl...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = df_normalized_train['text'].copy()\n",
    "new_sent = []\n",
    "i = 0\n",
    "for sentence in sentences:\n",
    "    new_sentence = re.sub('[0-9]+', '', sentence)\n",
    "    new_sent.append(new_sentence)\n",
    "    i += 1\n",
    "    \n",
    "df_normalized_train['text'] = new_sent\n",
    "df_normalized_train['text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "sentences = df_normalized_train['text'].copy()\n",
    "new_sent = []\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "    new_sent.append(stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85855     [öyle, böyle, bir, büyüklük, değil, bazı, hare...\n",
       "94508     [sıvı, detarjanların, hemen, hemen, hepsini, d...\n",
       "131917    [araçlar, hem, ülke, içinde, hem, de, ülke, dı...\n",
       "71544     [boschsiemens, yerli, üretimi, makinamızda, ra...\n",
       "41678     [topaklanmasi, cok, kotu, tane, kedimiz, var, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized_train['text'] = new_sent\n",
    "df_normalized_train['text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_str(text):\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74598c10dd144b6bbb6ac6b5a3deacfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=15272), Label(value='0 / 15272')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_normalized_train[\"text\"] = df_normalized_train[\"text\"].parallel_map(join_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_train = df_normalized_train.convert_dtypes(convert_string=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zemberek-NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-16 19:01:51,085 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 2.7105343341827393\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m normalizer \u001b[38;5;241m=\u001b[39m TurkishSentenceNormalizer(morphology)\n\u001b[0;32m     15\u001b[0m extractor \u001b[38;5;241m=\u001b[39m TurkishSentenceExtractor()\n\u001b[1;32m---> 16\u001b[0m spell_checker \u001b[38;5;241m=\u001b[39m \u001b[43mTurkishSpellChecker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmorphology\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\zemberek\\normalization\\turkish_spell_checker.py:31\u001b[0m, in \u001b[0;36mTurkishSpellChecker.__init__\u001b[1;34m(self, morphology, matcher, decoder)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmorphology \u001b[38;5;241m=\u001b[39m morphology\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m decoder:\n\u001b[1;32m---> 31\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mStemEndingGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmorphology\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m CharacterGraphDecoder(graph\u001b[38;5;241m.\u001b[39mstem_graph)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munigram_model: SmoothLM \u001b[38;5;241m=\u001b[39m SmoothLM\u001b[38;5;241m.\u001b[39mbuilder(\n\u001b[0;32m     34\u001b[0m         resource\u001b[38;5;241m=\u001b[39mresource_filename(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzemberek\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresources\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-unigram.slm\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mbuild()\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\zemberek\\normalization\\stem_ending_graph.py:21\u001b[0m, in \u001b[0;36mStemEndingGraph.__init__\u001b[1;34m(self, morphology)\u001b[0m\n\u001b[0;32m     19\u001b[0m endings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lines_from_resource()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mending_graph: CharacterGraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_ending_graph(endings)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem_graph: CharacterGraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_stem_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m stem_word_nodes: FrozenSet[Node] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem_graph\u001b[38;5;241m.\u001b[39mget_all_nodes())\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m stem_word_nodes:\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\zemberek\\normalization\\stem_ending_graph.py:40\u001b[0m, in \u001b[0;36mStemEndingGraph.generate_stem_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transition \u001b[38;5;129;01min\u001b[39;00m stem_transitions\u001b[38;5;241m.\u001b[39mget_transitions():\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(transition\u001b[38;5;241m.\u001b[39msurface) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m transition\u001b[38;5;241m.\u001b[39mitem\u001b[38;5;241m.\u001b[39mprimary_pos \u001b[38;5;241m!=\u001b[39m PrimaryPos\u001b[38;5;241m.\u001b[39mPunctuation:\n\u001b[1;32m---> 40\u001b[0m         \u001b[43mstem_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stem_graph\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\zemberek\\normalization\\character_graph.py:27\u001b[0m, in \u001b[0;36mCharacterGraph.add_word\u001b[1;34m(self, word, type_)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_word\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, type_: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Node:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\zemberek\\normalization\\character_graph.py:37\u001b[0m, in \u001b[0;36mCharacterGraph.add_\u001b[1;34m(self, current_node, index, word, type_)\u001b[0m\n\u001b[0;32m     35\u001b[0m child \u001b[38;5;241m=\u001b[39m current_node\u001b[38;5;241m.\u001b[39madd_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_index_counter\u001b[38;5;241m.\u001b[39mget_and_increment(), c, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     36\u001b[0m index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\zemberek\\normalization\\character_graph.py:37\u001b[0m, in \u001b[0;36mCharacterGraph.add_\u001b[1;34m(self, current_node, index, word, type_)\u001b[0m\n\u001b[0;32m     35\u001b[0m child \u001b[38;5;241m=\u001b[39m current_node\u001b[38;5;241m.\u001b[39madd_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_index_counter\u001b[38;5;241m.\u001b[39mget_and_increment(), c, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     36\u001b[0m index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping similar frames: CharacterGraph.add_ at line 37 (4 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\zemberek\\normalization\\character_graph.py:37\u001b[0m, in \u001b[0;36mCharacterGraph.add_\u001b[1;34m(self, current_node, index, word, type_)\u001b[0m\n\u001b[0;32m     35\u001b[0m child \u001b[38;5;241m=\u001b[39m current_node\u001b[38;5;241m.\u001b[39madd_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_index_counter\u001b[38;5;241m.\u001b[39mget_and_increment(), c, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     36\u001b[0m index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\zemberek\\normalization\\character_graph.py:33\u001b[0m, in \u001b[0;36mCharacterGraph.add_\u001b[1;34m(self, current_node, index, word, type_)\u001b[0m\n\u001b[0;32m     31\u001b[0m c \u001b[38;5;241m=\u001b[39m word[index]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m current_node\u001b[38;5;241m.\u001b[39madd_child(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_index_counter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_and_increment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, c, type_, word\u001b[38;5;241m=\u001b[39mword)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     child \u001b[38;5;241m=\u001b[39m current_node\u001b[38;5;241m.\u001b[39madd_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_index_counter\u001b[38;5;241m.\u001b[39mget_and_increment(), c, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\zemberek\\normalization\\character_graph.py:14\u001b[0m, in \u001b[0;36mAtomicCounter.get_and_increment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_and_increment\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     13\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount\n\u001b[1;32m---> 14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lock\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "from zemberek import (\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishSpellChecker\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "extractor = TurkishSentenceExtractor()\n",
    "spell_checker = TurkishSpellChecker(morphology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85855     öyle böyle bir büyüklük değil bazı hareketleri...\n",
       "94508     sıvı detarjanların hemen hemen hepsini denedim...\n",
       "131917    araçlar hem ülke içinde hem de ülke dışında sa...\n",
       "71544     boschsiemens yerli üretimi makinamızda rakibi ...\n",
       "41678     topaklanmasi cok kotu tane kedimiz var temizle...\n",
       "                                ...                        \n",
       "119879                   i̇ç anadolu bölgelerinde söylenir \n",
       "103694    babasının hediye ettiği gitar ile müzikle erke...\n",
       "131932    kapanış nedeni okur sayısının azalmasına ve so...\n",
       "146867                        savaş çıkınca askerden kaçtı \n",
       "121958    bilgi arama modelleri çeşitli unsurlardan etki...\n",
       "Name: text, Length: 152715, dtype: string"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized_train[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1 = df_normalized_train.iloc[:10000,:]\n",
    "df_train_2 = df_normalized_train.iloc[10000:20000,:]\n",
    "df_train_3 = df_normalized_train.iloc[20000:30000,:]\n",
    "df_train_4 = df_normalized_train.iloc[30000:40000,:]\n",
    "df_train_5 = df_normalized_train.iloc[40000:50000,:]\n",
    "df_train_6 = df_normalized_train.iloc[50000:60000,:]\n",
    "df_train_7 = df_normalized_train.iloc[60000:70000,:]\n",
    "df_train_8 = df_normalized_train.iloc[70000:80000,:]\n",
    "df_train_9 = df_normalized_train.iloc[80000:90000,:]\n",
    "df_train_10 = df_normalized_train.iloc[90000:100000,:]\n",
    "df_train_11 = df_normalized_train.iloc[100000:110000,:]\n",
    "df_train_12 = df_normalized_train.iloc[110000:120000,:]\n",
    "df_train_13 = df_normalized_train.iloc[120000:130000,:]\n",
    "df_train_14 = df_normalized_train.iloc[130000:140000,:]\n",
    "df_train_15 = df_normalized_train.iloc[140000:,:]\n",
    "dfs = [df_train_1,df_train_2,df_train_3,df_train_4,df_train_5,df_train_6,df_train_7,df_train_8,df_train_9,df_train_10,df_train_11,df_train_12,df_train_13,df_train_14,df_train_15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spells(text:str):\n",
    "\n",
    "    words = text.split(\" \")\n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            words[i] = spell_checker.suggest_for_word(word)[0]\n",
    "        except BaseException:\n",
    "            continue\n",
    "    return \" \".join(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964069987edb4dd2b71513e7ecf8f5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1000), Label(value='0 / 1000'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dfs):\n\u001b[0;32m----> 2\u001b[0m     dfs[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrect_spells\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/teknofest/lib/python3.11/site-packages/pandarallel/core.py:307\u001b[0m, in \u001b[0;36mparallelize_with_memory_file_system.<locals>.closure\u001b[0;34m(data, user_defined_function, *user_defined_function_args, **user_defined_function_kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m generation \u001b[38;5;241m=\u001b[39m count()\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    302\u001b[0m     (\n\u001b[1;32m    303\u001b[0m         worker_status \u001b[38;5;241m==\u001b[39m WorkerStatus\u001b[38;5;241m.\u001b[39mRunning\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m worker_status \u001b[38;5;129;01min\u001b[39;00m workers_status\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m ):\n\u001b[0;32m--> 307\u001b[0m     message: Tuple[\u001b[38;5;28mint\u001b[39m, WorkerStatus, Any] \u001b[38;5;241m=\u001b[39m \u001b[43mmaster_workers_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     worker_index, worker_status, payload \u001b[38;5;241m=\u001b[39m message\n\u001b[1;32m    309\u001b[0m     workers_status[worker_index] \u001b[38;5;241m=\u001b[39m worker_status\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mget\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/teknofest/lib/python3.11/multiprocessing/managers.py:822\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    819\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tls\u001b[38;5;241m.\u001b[39mconnection\n\u001b[1;32m    821\u001b[0m conn\u001b[38;5;241m.\u001b[39msend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id, methodname, args, kwds))\n\u001b[0;32m--> 822\u001b[0m kind, result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#RETURN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/teknofest/lib/python3.11/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/anaconda3/envs/teknofest/lib/python3.11/multiprocessing/connection.py:430\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 430\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/teknofest/lib/python3.11/multiprocessing/connection.py:395\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    393\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 395\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m read(handle, remaining)\n\u001b[1;32m    396\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i,df in tqdm(enumerate(dfs)):\n",
    "    dfs[i][\"text\"] = df[\"text\"].parallel_map(correct_spells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence(text):\n",
    "    return extractor.from_paragraph(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0830ff77fa3c4881b1edbe14f46d0815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MAP DONE:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i,df in tqdm(enumerate(dfs)):\n",
    "    dfs[i][\"text\"] = df[\"text\"].parallel_map(extract_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_train = df_normalized_train[df_normalized_train[\"text\"]!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_long_text(paragraph: str) -> str:\n",
    "    result = []\n",
    "    for sentence in paragraph:\n",
    "        result.append(normalizer.normalize(sentence))\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82885e0162d84fc8b51b3d0bb71c1f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MAP DONE:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\zemberek\\core\\hash\\multi_level_mphf.py:57: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  d = (d ^ a) * MultiLevelMphf.HASH_MULTIPLIER\n"
     ]
    }
   ],
   "source": [
    "for i,df in tqdm(enumerate(dfs)):\n",
    "    dfs[i][\"text\"] = df[\"text\"].parallel_map(normalize_long_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/musasina/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mı', 'aslında', 'ya', 'mü', 'sanki', 'gibi', 'da', 'birşey', 'diye', 'biri', 'çok', 'yani', 'ki', 'en', 'nerede', 've', 'hep', 'de', 'için', 'nasıl', 'az', 'mu', 'defa', 'ne', 'bazı', 'şu', 'veya', 'niye', 'nerde', 'hem', 'hepsi', 'acaba', 'ile', 'ise', 'nereye', 'neden', 'niçin', 'birkaç', 'ama', 'eğer', 'çünkü', 'her', 'kez', 'kim', 'belki', 'şey', 'siz', 'bu', 'o', 'hiç', 'biz', 'tüm', 'daha'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('turkish'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_stop_words(sentence):\n",
    "    return [word for word in sentence.split(\" \") if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "number sections must be larger than 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_normalized_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_normalized_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclear_stop_words\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\parallel_pandas\\core\\parallel_series.py:59\u001b[0m, in \u001b[0;36mseries_parallelize_map.<locals>.p_map\u001b[1;34m(data, arg, executor, na_action)\u001b[0m\n\u001b[0;32m     57\u001b[0m workers_queue \u001b[38;5;241m=\u001b[39m Manager()\u001b[38;5;241m.\u001b[39mQueue()\n\u001b[0;32m     58\u001b[0m split_size \u001b[38;5;241m=\u001b[39m get_split_size(n_cpu, split_factor)\n\u001b[1;32m---> 59\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[43mget_split_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m dill_arg \u001b[38;5;241m=\u001b[39m dill\u001b[38;5;241m.\u001b[39mdumps(arg)\n\u001b[0;32m     61\u001b[0m result \u001b[38;5;241m=\u001b[39m progress_imap(partial(_do_map, dill_arg\u001b[38;5;241m=\u001b[39mdill_arg,\n\u001b[0;32m     62\u001b[0m                                workers_queue\u001b[38;5;241m=\u001b[39mworkers_queue, na_action\u001b[38;5;241m=\u001b[39mna_action),\n\u001b[0;32m     63\u001b[0m                        tasks, workers_queue, n_cpu\u001b[38;5;241m=\u001b[39mn_cpu, total\u001b[38;5;241m=\u001b[39msplit_size, disable\u001b[38;5;241m=\u001b[39mdisable_pr_bar,\n\u001b[0;32m     64\u001b[0m                        show_vmem\u001b[38;5;241m=\u001b[39mshow_vmem, executor\u001b[38;5;241m=\u001b[39mexecutor, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mupper())\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\parallel_pandas\\core\\tools.py:61\u001b[0m, in \u001b[0;36mget_split_data\u001b[1;34m(df, axis, split_size, offset)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_split_data\u001b[39m(df, axis, split_size, offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     60\u001b[0m     split_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(split_size, df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m axis])\n\u001b[1;32m---> 61\u001b[0m     idx_split \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m iterate_by_df(df, idx_split, axis, offset)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tasks\n",
      "File \u001b[1;32mc:\\Users\\musasina\\anaconda3\\envs\\mfp\\Lib\\site-packages\\numpy\\lib\\shape_base.py:770\u001b[0m, in \u001b[0;36marray_split\u001b[1;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[0;32m    768\u001b[0m Nsections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(indices_or_sections)\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Nsections \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber sections must be larger than 0.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    771\u001b[0m Neach_section, extras \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(Ntotal, Nsections)\n\u001b[0;32m    772\u001b[0m section_sizes \u001b[38;5;241m=\u001b[39m ([\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    773\u001b[0m                  extras \u001b[38;5;241m*\u001b[39m [Neach_section\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    774\u001b[0m                  (Nsections\u001b[38;5;241m-\u001b[39mextras) \u001b[38;5;241m*\u001b[39m [Neach_section])\n",
      "\u001b[1;31mValueError\u001b[0m: number sections must be larger than 0."
     ]
    }
   ],
   "source": [
    "for i,df in tqdm(enumerate(dfs)):\n",
    "    dfs[i][\"text\"] = df[\"text\"].parallel_map(clear_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in tqdm(enumerate(dfs)):\n",
    "    dfs[i][\"text\"] = df[\"text\"].parallel_map(join_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_upper_punctuation(text):\n",
    "    return text.replace('\"', '').replace(\"’\", '').replace(\"'\", '').replace(\"”\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in tqdm(enumerate(dfs)):\n",
    "    dfs[i][\"text\"] = df[\"text\"].parallel_map(remove_upper_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_text(text):\n",
    "    return text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in tqdm(enumerate(dfs)):\n",
    "    dfs[i][\"text\"] = df[\"text\"].parallel_map(sep_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeyrek\n",
    "\n",
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "\n",
    "def lemmatize_sent(text):\n",
    "    return \" \".join([analyzer.lemmatize(word)[0][1][0] for word in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in tqdm(enumerate(dfs)):\n",
    "    dfs[i][\"text\"] = df[\"text\"].parallel_map(lemmatize_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in tqdm(enumerate(dfs)):\n",
    "    dfs[i][\"text\"] = df[\"text\"].parallel_map(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rare Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_train = pd.concat(dfs,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(df_normalized_train['text']).split()).value_counts()\n",
    "less_freq = list(freq[freq == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1 = df_normalized_train.iloc[:10000,:]\n",
    "df_train_2 = df_normalized_train.iloc[10000:20000,:]\n",
    "df_train_3 = df_normalized_train.iloc[20000:30000,:]\n",
    "df_train_4 = df_normalized_train.iloc[30000:40000,:]\n",
    "df_train_5 = df_normalized_train.iloc[40000:50000,:]\n",
    "df_train_6 = df_normalized_train.iloc[50000:60000,:]\n",
    "df_train_7 = df_normalized_train.iloc[60000:70000,:]\n",
    "df_train_8 = df_normalized_train.iloc[70000:80000,:]\n",
    "df_train_9 = df_normalized_train.iloc[80000:90000,:]\n",
    "df_train_10 = df_normalized_train.iloc[90000:100000,:]\n",
    "df_train_11 = df_normalized_train.iloc[100000:110000,:]\n",
    "df_train_12 = df_normalized_train.iloc[110000:120000,:]\n",
    "df_train_13 = df_normalized_train.iloc[120000:130000,:]\n",
    "df_train_14 = df_normalized_train.iloc[130000:140000,:]\n",
    "df_train_15 = df_normalized_train.iloc[140000:,:]\n",
    "dfs = [df_train_1,df_train_2,df_train_3,df_train_4,df_train_5,df_train_6,df_train_7,df_train_8,df_train_9,df_train_10,df_train_11,df_train_12,df_train_13,df_train_14,df_train_15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in tqdm(enumerate(dfs)):\n",
    "    dfs[i][\"text\"] = df[\"text\"].parallel_map(lambda x: \" \".join(x for x in x.split() if x not in less_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munidecode\u001b[39;00m\n\u001b[1;32m      3\u001b[0m turkish_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÇçĞğıİÖöŞşÜü\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m normal_chars \u001b[38;5;241m=\u001b[39m unidecode\u001b[38;5;241m.\u001b[39munidecode(turkish_chars)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "\n",
    "turkish_chars = \"ÇçĞğıİÖöŞşÜü\"\n",
    "normal_chars = unidecode.unidecode(turkish_chars)\n",
    "\n",
    "def change_turkish_chars(text:str):\n",
    "    for char,turkish_char in zip(normal_chars,turkish_chars):\n",
    "        text = text.replace(turkish_char,char)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in tqdm(enumerate(dfs)):\n",
    "    dfs[i][\"text\"] = df[\"text\"].parallel_map(change_turkish_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_train = pd.concat(dfs,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_train.to_csv(\"./datasets/train,csv\")\n",
    "df_test.to_csv(\"./datasets/test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
